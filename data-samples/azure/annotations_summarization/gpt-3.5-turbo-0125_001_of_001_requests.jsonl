{"custom_id": "azure-LN01-P8Z-summarize-summary", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Consider the following incident report was submitted on the azure portal.\nIncident Description: \"\"\"\n# Incident Summary:\nBetween 19:00 UTC on March 15, 2021 and 09:37 UTC on March 16, 2021, customers may have encountered errors performing authentication operations for any Microsoft services and third-party applications that depend on Azure Active Directory (Azure AD) for authentication.\n\n# Incident Impact: N/A\n\"\"\"\nYour task is to summarize this incident report. Focus on the following aspects of the incident:\n- The symptoms of the incident that lead to this incident report\n- References to external services or tools that contain relevant information.\n- Distinguishing features of the incident such as precise error codes, specifics from logs etc.\n- Context of the incident such as the name of the service, region, etc.\nYour summary should be at most 5-6 sentences and should be in third person. You must end your summary with <|endoftext|>.\nConcise Summary:"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-LN01-P8Z-summarize-root_cause", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Your task is to summarize the following root cause of an incident report.\n- Your summary must clearly state what the root cause of the incident was, and ignore timestamps in the summary.\n- Style: Brief, high-level.\n- Detail: States the root cause clearly and concisely without specifying broader impact or technical depth.\n- Avoid the common phrase \"The root cause of the incident was...\" or similar phrases.\nIncident Root Cause: \"\"\"\nAzure AD utilizes keys to support the use of OpenID and other Identity standard protocols for cryptographic signing operations. As part of standard security hygiene, an automated system, on a time-based schedule, removes keys that are no longer in use. Over the last few weeks, a particular key was marked as “retain” for longer than normal to support a complex cross-cloud migration. This exposed a bug where the automation incorrectly ignored that “retain” state, leading it to remove that particular key.\n\nMetadata about the signing keys is published by Azure AD to a global location in line with Internet Identity standard protocols. Once the public metadata was changed at 19:00 UTC on 15 March 2021, applications using these protocols with Azure AD began to pick up the new metadata and stopped trusting tokens/assertions signed with the key that was removed. At that point, end users were no longer able to access those applications.\n\"\"\"\nConcise Summary:"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-LN01-P8Z-summarize-mitigation", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Your task is to summarize the following mitigation steps taken to resolve an incident report.\n- Your summary must clearly state what the mitigation steps of the incident were, and ignore timestamps in the summary.\n- Style: Brief, high-level.\n- Detail: States the mitigation steps clearly and concisely without specifying broader impact or technical depth.\n- Avoid the common phrase \"The incident was resolved/mitigated by...\" or similar phrases.\nIncident Mitigation Steps: \"\"\"\nService telemetry identified the problem, and the engineering team was automatically engaged. At 19:35 UTC on 15 March 2021, we reverted deployment of the last backend infrastructure change that was in progress. Once the key removal operation was identified as the root cause, the key metadata was rolled back to its prior state at 21:05 UTC.\n\nApplications then needed to pick up the rolled back metadata and refresh their caches with the correct metadata. The time to mitigate for individual applications varies due to a variety of server implementations that handle caching differently. A subset of Storage resources experienced residual impact due to cached metadata. We deployed an update to invalidate these entries and force a refresh. This process completed and mitigation for the residually impacted customers was declared at 09:37 UTC on 16 March 2021.\n\nAzure AD is in a multi-phase effort to apply additional protections to the backend Safe Deployment Process (SDP) system to prevent a class of risks including this problem. The first phase does provide protections for adding a new key, but the remove key component is in the second phase which is scheduled to be finished by mid-year. A previous Azure AD incident occurred on September 28th, 2020 and both incidents are in the class of risks that will be prevented once the multi-phase SDP effort is completed.\n\"\"\"\nConcise Summary:\n"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-SNDL-NS8-summarize-summary", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Consider the following incident report was submitted on the azure portal.\nIncident Description: \"\"\"\n# Incident Summary:\nBetween 02:26 am and 03:40 am UTC on 04 Jul 2020, customers using Azure DevOps in multiple regions may have observed connectivity errors to DevOps services.\n\n# Incident Impact: N/A\n\"\"\"\nYour task is to summarize this incident report. Focus on the following aspects of the incident:\n- The symptoms of the incident that lead to this incident report\n- References to external services or tools that contain relevant information.\n- Distinguishing features of the incident such as precise error codes, specifics from logs etc.\n- Context of the incident such as the name of the service, region, etc.\nYour summary should be at most 5-6 sentences and should be in third person. You must end your summary with <|endoftext|>.\nConcise Summary:"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-SNDL-NS8-summarize-root_cause", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Your task is to summarize the following root cause of an incident report.\n- Your summary must clearly state what the root cause of the incident was, and ignore timestamps in the summary.\n- Style: Brief, high-level.\n- Detail: States the root cause clearly and concisely without specifying broader impact or technical depth.\n- Avoid the common phrase \"The root cause of the incident was...\" or similar phrases.\nIncident Root Cause: \"\"\"\nWe identified an inadvertent error with a configuration change in the back-end service which caused the outage.\n\"\"\"\nConcise Summary:"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-SNDL-NS8-summarize-mitigation", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Your task is to summarize the following mitigation steps taken to resolve an incident report.\n- Your summary must clearly state what the mitigation steps of the incident were, and ignore timestamps in the summary.\n- Style: Brief, high-level.\n- Detail: States the mitigation steps clearly and concisely without specifying broader impact or technical depth.\n- Avoid the common phrase \"The incident was resolved/mitigated by...\" or similar phrases.\nIncident Mitigation Steps: \"\"\"\nWe applied a configuration update which has fully mitigated the issue.\n\"\"\"\nConcise Summary:\n"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-NMB2-ND0-summarize-summary", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Consider the following incident report was submitted on the azure portal.\nIncident Description: \"\"\"\n# Incident Summary:\nBetween 02:41 and 14:30 UTC on 07 Jun 2022, a subset of customers experienced difficulties connecting to resources hosted in one particular Availability Zone (AZ) of the East US 2 region. This issue impacted a subset of storage and compute resources within one of the region’s three Availability Zones. As a result, Azure services with dependencies on resources in this zone also experienced impact.\n\nSince the vast majority of services that were impacted already support Availability Zones customers using always-available and/or zone-redundant services would have observed that this zone-specific incident did not affect the availability of their data and services. Five services (Application Insights, Log Analytics, Managed Identity Service, Media Services, and NetApp Files) experienced regional impact as a result of this zonal issue. These five services are already working towards enabling AZ support. Finally, while App Service instances configured to be zone-redundant would have stayed available, from the other AZs, control plane issues were observed regionally that may have prevented customers from performing service management operations during the impact window.\n\n# Incident Impact: N/A\n\"\"\"\nYour task is to summarize this incident report. Focus on the following aspects of the incident:\n- The symptoms of the incident that lead to this incident report\n- References to external services or tools that contain relevant information.\n- Distinguishing features of the incident such as precise error codes, specifics from logs etc.\n- Context of the incident such as the name of the service, region, etc.\nYour summary should be at most 5-6 sentences and should be in third person. You must end your summary with <|endoftext|>.\nConcise Summary:"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-NMB2-ND0-summarize-root_cause", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Your task is to summarize the following root cause of an incident report.\n- Your summary must clearly state what the root cause of the incident was, and ignore timestamps in the summary.\n- Style: Brief, high-level.\n- Detail: States the root cause clearly and concisely without specifying broader impact or technical depth.\n- Avoid the common phrase \"The root cause of the incident was...\" or similar phrases.\nIncident Root Cause: \"\"\"\nMicrosoft experienced an unplanned power oscillation in one of our datacenters within one of our Availability Zones in the East US 2 region. Components of our redundant power system created unexpected electrical transients, which resulted in the Air Handling Units (AHUs) detecting a potential fault, and therefore shutting themselves down pending a manual reset.\n\nThe electrical transients were introduced by anomalous component behavior within Uninterruptible Power Supply (UPS) modules, and cascaded throughout the datacenter electrical distribution system including electrical power supply to the mechanical cooling plant. As a result of the AHU self-protective shutdown, cooling to the datacenter was interrupted. Although the electrical transients did not impact our compute, networking, or storage infrastructure – which did not lose power – the mechanical cooling plant shutdown led to an escalating thermal environment, which induced protective shutdown of a subset of this IT infrastructure prior to the restoration of cooling.\n\nThorough detailed analysis has resulted in an adjustment to the UPS gain settings, preventing any further oscillations. These oscillations are the power equivalent of having a microphone too close to an amplifier – just as setting the volume too high can trigger a self-sustained sound oscillation, power oscillations can occur when the gain of the UPS is too high. The normal process of adding load to the UPS units results in an increase in gain and, in this case, the gain went high enough to cause the oscillations to occur. Adjusting the control gain setting lower in the UPS returns them to stable operation for all load values, preventing disruptions to any other infrastructure such as the AHUs.\n\nSubsets of equipment including network, storage, and compute infrastructure were automatically shut down, both to prevent damage to hardware and to protect data durability under abnormal temperatures. As a result, Azure resources and services with dependencies on these underlying resources experienced availability issues during the impact window. A significant factor of downstream service impact was that our storage infrastructure was amongst the hardware most affected by these automated power and thermal shutdowns. Eight storage scale units were significantly impacted – due to thermal shutdowns directly and/or loss of networking connectivity, itself due to thermal shutdowns of corresponding networking equipment. These scale units hosted Standard Storage including LRS/GRS redundant storage accounts, which in turn affected Virtual Machines (VMs) using Standard HDD disks backed by this storage, as well as other services and customers directly consuming blob/file and other storage APIs.\n\nThe platform continuously monitors input/output transactions from the VMs to their corresponding storage. So even if the scale unit running a VM’s underlying compute was operational, when transactions did not complete successfully within 120 seconds (inclusive of retries) the connectivity to its virtual disk is considered to be lost, and a temporary VM shutdown is initiated. Any workloads running on these impacted VMs, including first-party Azure services and third-party customer services, would have been impacted as their underlying hosts were either shut down by thermal triggers, or had their storage/networking impacted by the same.\n\"\"\"\nConcise Summary:"}], "logprobs": false, "temperature": null, "n": 1}}
{"custom_id": "azure-NMB2-ND0-summarize-mitigation", "method": "POST", "url": "/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "user", "content": "I want you to act as an expert software engineer. Your task is to summarize the following mitigation steps taken to resolve an incident report.\n- Your summary must clearly state what the mitigation steps of the incident were, and ignore timestamps in the summary.\n- Style: Brief, high-level.\n- Detail: States the mitigation steps clearly and concisely without specifying broader impact or technical depth.\n- Avoid the common phrase \"The incident was resolved/mitigated by...\" or similar phrases.\nIncident Mitigation Steps: \"\"\"\nAs soon as the AHUs shut themselves down as a result of the power disturbance, alerts notified our onsite datacenter operators. We deployed a team to investigate, who confirmed that the cooling units had shut themselves down pending manual intervention. Following our Standard Operating Procedure (SOP), the team attempted to perform manual resets on the AHUs, but these were not successful. Upon further investigation the onsite team identified that, due to the nature of this disturbance, recovering safely would require resetting the AHUs while running on backup power sources, to prevent the power oscillation pattern on the utility line from triggering a fault. This meant that two primary steps were required to recover – firstly, the impacted datacenter manually transferred from utility power to backup power sources, our onsite generators. By doing this, we changed the characteristics in the power lineup to obviate the creation of the oscillations. Secondly, the AHUs were then manually reset to recover them, which restored cooling to the datacenter.\n\nOnce temperatures returned to normal levels, some hardware including network switches needed to be manually power cycled to be brought back online. The network hardware and components serve different compute and storage resources for the scale units in this datacenter, including host instances for other applications and services. Onsite engineers then manually reviewed the status of various infrastructure components, to ensure that everything was working as intended.\n\nFollowing the restoration of most storage network connectivity, recovery activities included diagnosing and remediating any host nodes that had entered an unhealthy state due to loss of network, and triaging any other hardware failures to ensure that all storage infrastructure could be brought back online. Even after all storage nodes returned to a healthy state, two storage scale units still exhibited slightly lower API availability compared to before this incident. It was determined that this was caused by a limited number of storage software roles being in an unhealthy state – those roles were restarted, which restored full API availability for those scale units.\n\nSince our compute continuously monitors for Storage access, as storage/networking started recovering the compute VMs automatically started coming back up. This worked as expected in all cases expect on one scale unit, where the physical machines were shut down and did not recovery automatically. Since the VMs were originally down due to storage/networking issues, it was only detected once storage recovered, so we manually recycled the nodes to bring them back online. Upon investigation, an issue with the cluster power management unit prevented automatic recovery.\n\nTwo specific Azure services (ExpressRoute and Spatial Anchors) performed manual mitigations to fail customers over to use the other two Availability Zones within the region. Thus, while some impacted services recovered even earlier, full mitigation of this incident was declared at 14:30 UTC.\n\nAfter cooling was restored and infrastructure was brought back online, our onsite teams opted to leave the datacenter running on backup power sources during additional investigations and testing, both focused on the UPS gain setting. In consultation with our critical environment hardware suppliers, we ran comprehensive testing to confirm the relevant gain settings based on the amount of load across the system. After these settings were deployed, we have since returned the datacenter back to our normal utility power feed.\n\"\"\"\nConcise Summary:\n"}], "logprobs": false, "temperature": null, "n": 1}}
